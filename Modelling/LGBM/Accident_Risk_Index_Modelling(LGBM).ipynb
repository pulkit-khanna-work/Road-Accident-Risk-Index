{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INIGjad5WUAG",
        "outputId": "af6a9860-4030-4ff7-f221-972bd78988a6"
      },
      "id": "INIGjad5WUAG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd /content/drive/MyDrive/Colab Notebooks/Predict Accident Risk (Swiss Comp)/Modelling/LGBM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKqL7rkgWbY6",
        "outputId": "2e890cb2-7f95-4563-c667-634b72550cfd"
      },
      "id": "CKqL7rkgWbY6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Predict Accident Risk (Swiss Comp)/modelling/lgbm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUxOtAFM0cjY",
        "outputId": "f253d6b5-14b1-4a3d-9e28-ddb02697dc73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm"
      ],
      "id": "qUxOtAFM0cjY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1_lwXeJNz3e"
      },
      "outputs": [],
      "source": [
        "# Load Necessary Libraries\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "seed = 5"
      ],
      "id": "l1_lwXeJNz3e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjSp-MeaQ9Cn"
      },
      "source": [
        "# Load and Prepare Saved Data"
      ],
      "id": "HjSp-MeaQ9Cn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get parent directory (One level up)\n",
        "path_parent = os.path.dirname(os.getcwd())\n",
        "# Join path name as parent directory and file name\n",
        "path_file_train = os.path.join(path_parent, 'accident_train.csv')\n",
        "path_file_test = os.path.join(path_parent, 'accident_test.csv')"
      ],
      "metadata": {
        "id": "avnPN32VmrTT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "avnPN32VmrTT"
    },
    {
      "cell_type": "code",
      "source": [
        "## Load preprocessed file\n",
        "\n",
        "Xy_train = pd.read_csv(path_file_train, sep = ',')\n",
        "Xy_test = pd.read_csv(path_file_test, sep = ',')"
      ],
      "metadata": {
        "id": "sagqkb_8nP1_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sagqkb_8nP1_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkrMV6Dk5SZI"
      },
      "source": [
        "### Prepare Data for Modelling"
      ],
      "id": "HkrMV6Dk5SZI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N0RWyEX5ZkL"
      },
      "source": [
        "We will model our data such that we will first predict 'Number_of_Casualties' by using regression or classification methods (Columns 'Accident_ID'and 'postcode' removed during training) and after training, calculate 'Accident Risk Index' using the calculated predicted values, 'Accident Id' and 'postcode'.\n",
        "\n",
        "Note: We can also model for 'Accident Risk Index' and predict its value directly. However, for that that data needs to be stratified split across 'postcode' as well and all 'postcode' type which contain a unit/single data point/value need to be either upsampled or manually added to training data set (Stratified Split won't work for datasets with unit value counts/frequency)"
      ],
      "id": "3N0RWyEX5ZkL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6j2sZe02bVw"
      },
      "outputs": [],
      "source": [
        "def prepare_model_data(data, output_col, remove_col):\n",
        "  X = data.drop(columns = output_col + remove_col)\n",
        "  y = data['Number_of_Casualties']\n",
        "  y = y.values.ravel()\n",
        "  return X, y"
      ],
      "id": "W6j2sZe02bVw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGuAdHf92bVy"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = prepare_model_data(Xy_train, ['Number_of_Casualties'], \n",
        "                                      ['Accident_ID', 'postcode'])\n",
        "X_test, y_test = prepare_model_data(Xy_test, ['Number_of_Casualties'], \n",
        "                                      ['Accident_ID', 'postcode'])"
      ],
      "id": "ZGuAdHf92bVy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he1trTRuN5fx",
        "outputId": "9e016e53-1b20-4e6a-c8d4-9c27dafcd2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(335118, 23) (335118,)\n",
            "(143623, 23) (143623,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "id": "he1trTRuN5fx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YAZdrcn-fk_"
      },
      "source": [
        "We will next define a function to prepare the data so as to calculate 'Accident Risk Index'"
      ],
      "id": "_YAZdrcn-fk_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDf3ZYJJ-7pB"
      },
      "outputs": [],
      "source": [
        "def cal_ari(data):\n",
        "  # we need to calculate Accident_Risk_Index as sum(Number_of_casualities)/count(Accident_ID)\n",
        "  grouped_train = data.groupby('postcode') # group by postcode\n",
        "  \n",
        "  #create aggregation functions\n",
        "  aggregations = {'Number_of_Casualties': [np.mean],\n",
        "                  'Accident_ID': [np.count_nonzero]}\n",
        "  aggregated_ = grouped_train.agg(aggregations)\n",
        "  \n",
        "  # formula mentioned above is used to calculate <Accident_Risk_Index>\n",
        "  aggregated_['Accident_risk_index'] = aggregated_['Number_of_Casualties']['mean']/aggregated_['Accident_ID']['count_nonzero']\n",
        "  return aggregated_"
      ],
      "id": "sDf3ZYJJ-7pB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0_QRHiN-8k9"
      },
      "source": [
        "Preparing Test Data with 'Accident Risk Index'"
      ],
      "id": "i0_QRHiN-8k9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hg4MHX9-7pD"
      },
      "outputs": [],
      "source": [
        "ari_test = cal_ari(Xy_test)"
      ],
      "id": "9hg4MHX9-7pD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA94I3ytjcrh"
      },
      "source": [
        "# Metric"
      ],
      "id": "AA94I3ytjcrh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjhd600KhxPj"
      },
      "source": [
        "We will use mean squared error metric to measure performance of our models"
      ],
      "id": "fjhd600KhxPj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P83Fx1BmlYNi"
      },
      "source": [
        "## Unbalanced Classes\n",
        "\n",
        "### Undersampling and Oversampling"
      ],
      "id": "P83Fx1BmlYNi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea3KxLYplppa"
      },
      "source": [
        "Here we will demonstrate how to perform combination of oversampling and downsampling using SMOTE and Tomek-Links from 'imblearn' library to balance out our classes. \n",
        "\n",
        "Note: \n",
        "\n",
        "  * We can weight out our classes when performing \n",
        "  modelling instead of using the above method. \n",
        "  \n",
        "  * We can build separate models and/or model within model in addition to the above methods when appropriate\n",
        "\n",
        "  * Downsampling leads to loss of data, however\n",
        "  combination of downsampling and oversampling can \n",
        "  give much better results "
      ],
      "id": "Ea3KxLYplppa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_P9Px2KtRdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c4bec2-8065-41de-f825-9d9e134394c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (3.1.0)\n",
            "Installing collected packages: imbalanced-learn\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.8.1\n",
            "    Uninstalling imbalanced-learn-0.8.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.8.1\n",
            "Successfully installed imbalanced-learn-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade imbalanced-learn"
      ],
      "id": "r_P9Px2KtRdM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5bdGAC-L6c"
      },
      "outputs": [],
      "source": [
        "# Need labels for output if  you want to use custom strategy\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# combination sampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks"
      ],
      "id": "zT5bdGAC-L6c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEEBi56WDwaC"
      },
      "source": [
        "We need to encode out output before we can use our sampler from 'imblearn' library."
      ],
      "id": "NEEBi56WDwaC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67N-5X5C-Oou"
      },
      "outputs": [],
      "source": [
        "# label encode the target variable\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)"
      ],
      "id": "67N-5X5C-Oou"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHP7BEl1VXnq"
      },
      "source": [
        "Our dataset contains only numeric data types. We can use SMOTE, ADASYN, BorderlineSMOTE etc as oversampling methods. However, it is better to use a combination of oversampling and undersampling to achieve better results.\n",
        "\n",
        "Here we will use 'SMOTETomek' combination method which uses SMOTE for over-sampling and Tomek links for cleaning."
      ],
      "id": "gHP7BEl1VXnq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Note 1: We can also use sklearn resample to balance out our classes, however we should note that SMOTE method creates synthetic(new) data points.\n",
        "\n",
        "* Note 2: Random undersampling, although simple and effective, has the drawback of removing data points without any concern for how useful or important they might be in determining the decision boundary between the classes. 'TomekLinks', 'Edited Nearest Neighbours' help in overcoming this drawback."
      ],
      "metadata": {
        "id": "41J4pdpMyreU"
      },
      "id": "41J4pdpMyreU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1_lGDDGVwp-"
      },
      "source": [
        "Create a dictionary to determine in what ratio/amount you want to split"
      ],
      "id": "h1_lGDDGVwp-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtB1m2GdWtoK",
        "outputId": "ff7af267-6dec-4f16-8642-b6c12d98c75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value_counts: [230386  69170  24779   6523   4260] \n",
            "labels: [0 1 2 3 4]\n"
          ]
        }
      ],
      "source": [
        "print(\"value_counts: {} \\nlabels: {}\".format(np.bincount(y_train_enc), np.unique(y_train_enc)))"
      ],
      "id": "DtB1m2GdWtoK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEaZGyk3Ztgn"
      },
      "source": [
        "We see that the data us heavily unbalanced, and thus won't over-sample a lot so as to balance out the data. We would use sampling in combination with weighing method as oversampling using SMOTE(Synthetic points) in excess might add an unnecessary bias.\n",
        "Also, note that we cannot control the amount to sample (reduce) with TomekLinks which comes under clean-sampling method and would be done automatically by giving class labels that we want to undersample/reduce."
      ],
      "id": "jEaZGyk3Ztgn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yju-6U8PVNxE"
      },
      "outputs": [],
      "source": [
        "# strategy for SMOTE(over-sampling) for output percentage label wise as a dictionary\n",
        "strategy_smote = {1:150000, 2:75000, 3:25000, 4:25000}\n",
        "# strategy for tomeklinks(clean-sampling) should be a list of labels \n",
        "strategy_tomek = [0]"
      ],
      "id": "yju-6U8PVNxE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-A807OV6zE"
      },
      "source": [
        "Create samples using 'SMOTETomek' using the above parameter settings"
      ],
      "id": "c8-A807OV6zE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR68G9wC_1HV"
      },
      "outputs": [],
      "source": [
        "comb_sample = SMOTETomek(smote = SMOTE(sampling_strategy = strategy_smote), \n",
        "                 tomek = TomekLinks(sampling_strategy = strategy_tomek),\n",
        "                 random_state = seed)\n",
        "\n",
        "X_comb, y_comb = comb_sample.fit_resample(X_train, y_train_enc)"
      ],
      "id": "eR68G9wC_1HV"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New Value Counts: {}\".format(np.bincount(y_comb)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOF4dGRMVlwh",
        "outputId": "706f8c33-1588-4ae0-92e1-2f48a4b6a8ff"
      },
      "id": "QOF4dGRMVlwh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Value Counts: [202657 150000  75000  25000  25000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New Distribution Percentage: {}\".format((np.round(np.bincount(y_comb)/len(y_comb), 4) * 100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh1dqS6hWZqW",
        "outputId": "3bf62c07-7a7a-4a8c-c867-8fb2baba1397"
      },
      "id": "Hh1dqS6hWZqW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Distribution Percentage: [42.43 31.4  15.7   5.23  5.23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYsZWv_hSPe_"
      },
      "outputs": [],
      "source": [
        "# Converting encoding back to labels\n",
        "\n",
        "y_comb = pd.Series(le.inverse_transform(y_comb), name = 'Number_of_Casualties')"
      ],
      "id": "xYsZWv_hSPe_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er3E8CWeB7pS"
      },
      "outputs": [],
      "source": [
        "Xy_train = pd.concat([X_comb, y_comb], axis = 1)"
      ],
      "id": "er3E8CWeB7pS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRTEpnNp5uj"
      },
      "source": [
        "# Training"
      ],
      "id": "VdRTEpnNp5uj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frkdEOpy1Fcx"
      },
      "source": [
        "We will demonstrate now how to use LightGBM algorithm for training and building our model"
      ],
      "id": "frkdEOpy1Fcx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV9RifpT1ccQ"
      },
      "source": [
        "Here we will use lightgbm classifier for training and evaluating our model.\n",
        "\n",
        "* Note 1: We can also use lightgbm regressor for training and evaluating our model, however since our output is discrete in nature with only 5 values, lightgbm classifier would be the preferred method. Also, with lightgbm regressor we would be more intereted in its distribution and might have use transformation, resampling techniques, tweedie regressor(weighted regression) to balanced out our data.\n",
        "\n",
        "* Note 2: Poisson regression is a suitable candiate for modelling data with discrete response variable and we will demonstrate later a Poisson GAM.\n",
        "\n"
      ],
      "id": "uV9RifpT1ccQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TskXmg_o1TDY"
      },
      "source": [
        "### Baseline LightGBM"
      ],
      "id": "TskXmg_o1TDY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ls0PDEJ0mIg"
      },
      "outputs": [],
      "source": [
        "model_lgb = LGBMClassifier(class_weight = 'balanced', objective = 'multiclass',\n",
        "                           random_state = seed)"
      ],
      "id": "7ls0PDEJ0mIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBO9346G0n60"
      },
      "outputs": [],
      "source": [
        "lgb = model_lgb.fit(X_train, y_train)"
      ],
      "id": "BBO9346G0n60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecE1mLqn2Tqg"
      },
      "outputs": [],
      "source": [
        "model_predict = lgb.predict(X_test)"
      ],
      "id": "ecE1mLqn2Tqg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error for 'Number of Casualties'"
      ],
      "metadata": {
        "id": "l72jT_0QYf3r"
      },
      "id": "l72jT_0QYf3r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uww2wxEr7pOa",
        "outputId": "fcb8d814-6e34-4e2c-d21f-56e4683dc648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2.070899246319447\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean Squared Error: {}\".format(mean_squared_error(y_test, model_predict,  squared = False)))"
      ],
      "id": "Uww2wxEr7pOa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Accident Risk Index for predicted values"
      ],
      "metadata": {
        "id": "4RggxmRAYrc7"
      },
      "id": "4RggxmRAYrc7"
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = copy.deepcopy(Xy_test)\n",
        "\n",
        "pred_test['Number_of_Casualties'] = model_predict"
      ],
      "metadata": {
        "id": "qKHV9ingxwDJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qKHV9ingxwDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "ari_pred = cal_ari(pred_test)"
      ],
      "metadata": {
        "id": "iYIvpU12xwOZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "iYIvpU12xwOZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error for 'Accident Risk Index'"
      ],
      "metadata": {
        "id": "Q4QiExmH-si9"
      },
      "id": "Q4QiExmH-si9"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean Squared Error: {}\".format(mean_squared_error(ari_test['Accident_risk_index'], \n",
        "                                                         ari_pred['Accident_risk_index'],\n",
        "                                                         squared = False)))"
      ],
      "metadata": {
        "id": "mbr5yy6E-eaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d047d3a0-ec7f-45be-da2b-1a258be91dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1.4931607075321371\n"
          ]
        }
      ],
      "id": "mbr5yy6E-eaq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOoHFhyu-GuG"
      },
      "source": [
        "### Tuning LGBM Hyperparameters"
      ],
      "id": "fOoHFhyu-GuG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9O-FyTUrwdA"
      },
      "source": [
        "Now we will start tuning our LGBM classifier model.\n",
        "Here we will demonstrate how to estimate 'number of trees' and 'learning rate' using Grid Search.\n",
        "We can fine tune in a similar way as we did with XGBoost.\n",
        "\n",
        "* Note 1: Optuna is another library useful for the same and gives better results in more efficient way.\n",
        "\n",
        "* Note 2: We should first focus on feature engineering and trying other models first to check for improvement in score and then fine tune."
      ],
      "id": "z9O-FyTUrwdA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgmvIjfI7r6k"
      },
      "outputs": [],
      "source": [
        "# define evaluation procedure\n",
        "skf = StratifiedKFold(n_splits = 5, random_state = seed, shuffle = True)"
      ],
      "id": "DgmvIjfI7r6k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j_gDFYCt0MQ"
      },
      "source": [
        "### Tuning Number of Trees/Estimators"
      ],
      "id": "_j_gDFYCt0MQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63OYNQ_VdyPd"
      },
      "outputs": [],
      "source": [
        "param_grid_custom = {'n_estimators':[100, 250, 500, 1000],\n",
        "                     'learning_rate':[0.1, 0.5, 0.01]}"
      ],
      "id": "63OYNQ_VdyPd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg1BoU3L2d6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fd2083-e3de-48ec-ad8b-f71f280b7d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best acc score: -2.026999 using {'learning_rate': 0.5, 'n_estimators': 1000} \n"
          ]
        }
      ],
      "source": [
        "GR = GridSearchCV(estimator = model_lgb, param_grid = param_grid_custom, \n",
        "                  scoring = 'neg_mean_squared_error', cv = skf, refit = True, \n",
        "                  n_jobs = -1)    \n",
        "GR_lgbm = GR.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best acc score: %f using %s \"%(GR_lgbm.best_score_, GR_lgbm.best_params_))"
      ],
      "id": "hg1BoU3L2d6U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUMTVOv9_q82"
      },
      "source": [
        "### Evaluating LGBMBoost"
      ],
      "id": "NUMTVOv9_q82"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEuq3_eS2gXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8f72b8-5474-4e29-8083-3d54561f48b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Casualties (Mean Squared Error): 1.5078940466662558\n"
          ]
        }
      ],
      "source": [
        "model_predict = GR_lgbm.predict(X_test)\n",
        "\n",
        "print(\"Number of Casualties (Mean Squared Error): {}\".format((mean_squared_error(y_test, \n",
        "                                                                           model_predict,\n",
        "                                                                           squared = False))))"
      ],
      "id": "aEuq3_eS2gXN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Accident Risk Index for predicted values"
      ],
      "metadata": {
        "id": "nW_DrRDUaBkm"
      },
      "id": "nW_DrRDUaBkm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6de7OMG2n9y"
      },
      "outputs": [],
      "source": [
        "pred_test = copy.deepcopy(Xy_test)\n",
        "\n",
        "pred_test['Number_of_Casualties'] = model_predict"
      ],
      "id": "a6de7OMG2n9y"
    },
    {
      "cell_type": "code",
      "source": [
        "ari_pred = cal_ari(pred_test)"
      ],
      "metadata": {
        "id": "hhgB0SU3aCeF"
      },
      "id": "hhgB0SU3aCeF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RjYCOYuH2I5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700b7651-c8d8-40c3-89c1-4254d7401cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1.0493149452467978\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean Squared Error: {}\".format(mean_squared_error(ari_test['Accident_risk_index'], \n",
        "                                                         ari_pred['Accident_risk_index'],  \n",
        "                                                         squared = False)))"
      ],
      "id": "9RjYCOYuH2I5"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Predict_Accident_Risk_Modelling (LGBM).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}